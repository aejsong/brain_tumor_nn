{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Activation, Conv3D, MaxPooling3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 out of 1032\n",
      "200 out of 1032\n",
      "300 out of 1032\n",
      "400 out of 1032\n",
      "500 out of 1032\n",
      "600 out of 1032\n",
      "700 out of 1032\n",
      "800 out of 1032\n",
      "900 out of 1032\n",
      "1000 out of 1032\n"
     ]
    }
   ],
   "source": [
    "hgg_img_data_train = []\n",
    "counter = 0\n",
    "\n",
    "for file in os.listdir('./data/bet_processed/'):\n",
    "    if file.endswith('.nii.gz'):\n",
    "        file_path = './data/bet_processed/' + file\n",
    "        img = nib.load(file_path)\n",
    "        array_flat = img.get_data() / 255\n",
    "        hgg_img_data_train.append(array_flat)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter%100 == 0:\n",
    "            print(f\"{counter} out of {len(os.listdir('./data/bet_processed/'))-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgg_y = np.ones(shape=len(hgg_img_data_train), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgg_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 out of 303\n",
      "200 out of 303\n",
      "300 out of 303\n"
     ]
    }
   ],
   "source": [
    "lgg_img_data_train = []\n",
    "counter = 0\n",
    "\n",
    "for file in os.listdir('./data/lgg_bet_processed/'):\n",
    "    if file.endswith('.nii.gz'):\n",
    "        file_path = './data/lgg_bet_processed/' + file\n",
    "        img = nib.load(file_path)\n",
    "        array_flat = img.get_data() / 255\n",
    "        lgg_img_data_train.append(array_flat)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter%100 == 0:\n",
    "            print(f\"{counter} out of {len(os.listdir('./data/lgg_bet_processed/'))-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgg_y = np.zeros(shape=len(lgg_img_data_train), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(hgg_img_data_train + lgg_img_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(list(hgg_y) + list(lgg_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7724550898203593"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of Data that's Target: 1\n",
    "sum(y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 240, 155)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1002, 240, 240, 155)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REALLY BASIC 3D-CNN JUST TO SEE\n",
    "\n",
    "cnn = Sequential()\n",
    "\n",
    "cnn.add(Conv3D(\n",
    "    filters=6,\n",
    "    kernel_size=(3,3,3), # height/width of filter\n",
    "    activation='relu',\n",
    "    input_shape=(240, 240, 155, 1)))\n",
    "\n",
    "cnn.add(MaxPooling3D(\n",
    "    pool_size=(2,2,2)\n",
    "))\n",
    "\n",
    "cnn.add(Conv3D(\n",
    "    filters= 16,\n",
    "    kernel_size= (3,3,3),\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "cnn.add(MaxPooling3D(\n",
    "    pool_size=(2,2,2)\n",
    "))\n",
    "\n",
    "cnn.add(Flatten())\n",
    "\n",
    "cnn.add(Dense(\n",
    "    units=128,\n",
    "    activation='relu'\n",
    "))\n",
    "\n",
    "# Output Layer\n",
    "cnn.add(Dense(\n",
    "    units=2,\n",
    "    activation='softmax'\n",
    "))\n",
    "\n",
    "cnn.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(np.array(X_train), y_train,\n",
    "                 batch_size=512,\n",
    "                 validation_data=(np.array(X_test), y_test),\n",
    "                 epochs=5,\n",
    "                 verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Work/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \n",
      "/Users/Work/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]]\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]]\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]]\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]]\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]]\n"
     ]
    }
   ],
   "source": [
    "test_train = []\n",
    "counter = 0\n",
    "\n",
    "for file in os.listdir('./data/bet_processed/')[:5]:\n",
    "    if file.endswith('.nii.gz'):\n",
    "        file_path = './data/bet_processed/' + file\n",
    "        img = nib.load(file_path)\n",
    "        array_flat = img.get_data() / 0\n",
    "        print(array_flat)\n",
    "#         test_train.append(array_flat)\n",
    "        \n",
    "#         counter += 1\n",
    "#         if counter%100 == 0:\n",
    "#             print(f\"{counter} out of {len(os.listdir('./data/bet_processed/')[:5])-1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
